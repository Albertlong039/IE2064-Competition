knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(modelr)
library(e1071)
cwd <- dirname(rstudioapi::getSourceEditorContext()$path)
data <- read.csv(
paste(cwd,"/competition-data.csv",sep=""),
stringsAsFactors = FALSE
)
total_na <- function(x) sum(is.na(x))
CPM_na <- sort(data %>% summarise(across(everything(), total_na)),decreasing=TRUE)
CPM_na[,1:6]
skewness(data$outcome)
data <- data %>% filter(outcome >0)
skewness(data$outcome)
data <- data %>% filter(outcome >0)
skewness(data$outcome)
predictors_only <- select(data, -outcome)
df_predictors_only <- data.frame(predictors_only)
preprocessing_fit <- preProcess(
df_predictors_only,
method = c("BoxCox", "center", "scale"))
preprocessing_fit
transformed_predictors <- predict(preprocessing_fit, df_predictors_only)
transformed_predictors$outcome = data$outcome
ggplot(transformed_predictors, aes(y= outcome)) + geom_boxplot()
sapply(transformed_predictors, class)
remove_these_predictors <- nearZeroVar(transformed_predictors)
preprocessed <- transformed_predictors[,-remove_these_predictors]
ncol(preprocessed)
predictors_only <- data.frame(preprocessed %>% select(-outcome))
predictors_only_correlation_matrix <- cor(predictors_only)
cutoff <- 0.95
names_of_predictors_to_remove <- findCorrelation(
predictors_only_correlation_matrix, names=TRUE, cutoff=cutoff)
removed_predictors <- preprocessed %>%
select(-all_of(names_of_predictors_to_remove))
ncol(removed_predictors)
set.seed(100)
trainData <- removed_predictors
folds <- createFolds(trainData$outcome, k = 10, returnTrain = TRUE)
ctrl <- trainControl(method = "cv", index = folds)
# Non-linear models
# Support Vector Machines (SVM)
set.seed(123)
svm_fit <- train(outcome ~ ., data = trainData, method = "svmRadial", trControl =
ctrl)
# Neural Nets
set.seed(123)
nnet_fit <- train(outcome ~ ., data = trainData, method = "nnet", trControl =
ctrl)
# Multivariate Adaptive Regression Splines (MARS)
set.seed(123)
mars_fit <- train(outcome ~ ., data = trainData, method = "earth", trControl =
ctrl)
# K-nearest
set.seed(123)
knn_fit = train(outcome~ ., data = trainData, method = "knn", trControl =
ctrl, preProc = c("center", "scale"))
Non_linear_models <- list(min(svm_fit$results$RMSE),
min(nnet_fit$results$RMSE),
min(mars_fit$results$RMSE),
min(knn_fit$results$RMSE))
Non_linear_models
set.seed(100)
lmTune <- train(outcome ~ .,data = trainData,
method = "lm",
trControl = ctrl)
set.seed(100)
plsTune <- train(outcome ~ .,data = trainData,
method = "pls",
tuneGrid = expand.grid(ncomp = 1:50),
trControl = ctrl)
set.seed(100)
pcrTune <- train(outcome ~ .,data = trainData,
method = "pcr",
tuneGrid = expand.grid(ncomp = 1:50),
trControl = ctrl)
set.seed(100)
ridgeGrid <- expand.grid(lambda = seq(0, .1, length = 10))
ridgeTune <- train(outcome ~ .,data = trainData,
method = "ridge",
tuneGrid = ridgeGrid,
trControl = ctrl,
preProc = c("center", "scale")
)
set.seed(100)
lassoGrid <- expand.grid(lambda = c(0),
fraction = seq(.1, 1, length = 15))
lassoTune <- train(outcome ~ .,data = trainData,
method = "enet",
tuneGrid = lassoGrid,
trControl = ctrl,
preProc = c("center", "scale")
)
set.seed(100)
enetGrid <- expand.grid(lambda = seq(0.0, 0.02, length = 5),
fraction = seq(.1, 1, length = 5))
enetTune <- train(outcome ~ .,data = trainData,
method = "enet",
tuneGrid = enetGrid,
trControl = ctrl,
preProc = c("center", "scale")
)
Regression_models <- list(min(lmTune$results$RMSE),
min(plsTune$results$RMSE),
min(pcrTune$results$RMSE),
min(ridgeTune$results$RMSE),
min(lassoTune$results$RMSE),
min(enetGrid$results$RMSE))
set.seed(100)
enetGrid <- expand.grid(lambda = seq(0.0, 0.02, length = 5),
fraction = seq(.1, 1, length = 5))
enetTune <- train(outcome ~ .,data = trainData,
method = "enet",
tuneGrid = enetGrid,
trControl = ctrl,
preProc = c("center", "scale")
)
Regression_models <- list(min(lmTune$results$RMSE),
min(ridgeTune$results$RMSE),
min(lassoTune$results$RMSE),
min(enetGrid$results$RMSE))
Regression_models
set.seed(100)
enetGrid <- expand.grid(lambda = seq(0.0, 0.02, length = 5),
fraction = seq(.1, 1, length = 5))
enetTune <- train(outcome ~ .,data = trainData,
method = "enet",
tuneGrid = enetGrid,
trControl = ctrl,
preProc = c("center", "scale")
)
Regression_models <- list(min(lmTune$results$RMSE),
min(ridgeTune$results$RMSE),
min(lassoTune$results$RMSE),
min(enetTune$results$RMSE))
Regression_models
# Random forests
set.seed(123)
mtryGrid <- data.frame()
rf_fit = train(outcome ~ ., data = trainData, method = "rf", ntree=50, importance = TRUE)
summary(rf_fit$finalModel)
Trees_models <- list(min(tree_fit$results$RMSE),
min(rf_fit$results$RMSE))
# Trees
# Regression tree
set.seed(123)
tree_fit = train(outcome ~ ., data = trainData, method = "rpart", trControl = ctrl)
summary(tree_fit$finalModel)
Trees_models <- list(min(tree_fit$results$RMSE),
min(rf_fit$results$RMSE))
Trees_models
Trees_models <- list(min(tree_fit$results$RMSE),
min(rf_fit$results$RMSE))
Trees_models
rf_fit$results
cwd <- dirname(rstudioapi::getSourceEditorContext()$path)
data2 <- read.csv(
paste(cwd,"/competition-test-x-values.csv",sep=""),
stringsAsFactors = FALSE
)
predict(data2,rf_fit)
predict(data2,model = rf_fit)
predictions <- predict(rf_fit, data2)
predictions <- predict(rf_fit, data2)
predictions
predictions <- predict(rf_fit, data2)
data2$outcome <- predictions
View(data)
View(data2)
write.csv(data2, paste(cwd,"/results.csv",sep=""), row.names = FALSE)
